{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks with Keras (Work In Progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Do you remember how do we generate random values starting from a uniformely distributed random variable $U$? We can do that using the inverse transform techinique, which states that $X=F^{-1}(U)$ is a random variable with [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (CDF) given by $F_X(X) = F$.\n",
    "\n",
    "For example, assume we want to generate an exponentialy distributed random variable. Wikipedia gives us the following facts:\n",
    "1. Probability disbutition function (PDF):\n",
    "\\begin{equation}\n",
    "f_X(x) = \\lambda e^{-\\lambda x}\n",
    "\\end{equation}\n",
    "2. CDF:\n",
    "\\begin{equation}\n",
    "F_X (x) = \\int_{-inf}^x f_X(z)dz = 1-e^{-\\lambda x}\n",
    "\\end{equation}\n",
    "3. Inverting that CDF we have:\n",
    "\\begin{equation}\n",
    "F_X^{-1}(u) = -\\frac{1}{\\lambda}log(1-u)\n",
    "\\end{equation}\n",
    "\n",
    "Let's check that with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAk4AAADSCAYAAAC8Yk/kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAHfdJREFUeJzt3Xu0JWV55/HvDxpEREECaW6tEAeMmEkgMUhEYpMQBgwB\n",
       "M1kRiCRomCxXIErugjORVjPmMiuGZA0yiQLpIMEQGQjGGy3SiblIi3ZzawigdKSRbgg3JSYOyDN/\n",
       "VDVujuecXedS+5yz+/tZq9apXbuq3rf25TnPfuutt1JVSJIkabgdFroCkiRJS4WJkyRJUkcmTpIk\n",
       "SR2ZOEmSJHVk4iRJktSRiZMkSVJHJk6alSQXJfkfA49/McnWJF9N8sKFrJskLTZJ3pDkkx3XXZXk\n",
       "sr7rpNmJ4zhprpLsBDwOHFFVty10fSRpISU5EPgSsKyqnp7F9ucD/6mqfnaeq6Z5YIuT5sM+wC7A\n",
       "HTPdMK35r5IkLbjZxjZj4iJm4rQdS/J0ku8aePxnSd7dzq9MsjnJr7an4L6S5I0T101yMHBnu/ix\n",
       "JJ9qn39Vks8leSzJuiQ/NLDt2iS/neQfgCeA72rr8otJ7m5P970ryUuS/FO7jw+1LVuSxlSS/ZJc\n",
       "leTBJF9K8pYkeya5L8mJ7Tq7Jbknyent4z9L8n+SXNfGjrVJXjSwz2Gx6F1J/r7d9pNJvmPg+SOT\n",
       "/GOSR5NsSPKajtv+Xfv3sfa5I5O8MclnBrb/oyRfTvJ4kpuSvLqfV1XzzcRJg6qdtlkOvADYDzgT\n",
       "uDDJ7oPrVtXdwKHtst2r6tgkewIfBS4A9gTeC3x0Qt+n04H/Bjwf+HK77DjgcOBI4G3A+4HTgBcB\n",
       "/7mdlzSGkuwAfARYTxNzfhT4ZeAVwM8D70+yN/CHwBeq6oMDm/8M8C5gL2ADcHm7zy6x6DTgjcB3\n",
       "AjsDv95uuz/wN8C7quqF7fKrBhOrqbYFjm7/7l5VL6iqz05yyOuA7wNeCPwF8FdJdu7wUmmBmThp\n",
       "osEm4idpgsY3q+rjNK1DL51k3YnNyj8O/HNVXV5VT1fVh2hapU5qny/gz6rqjvb5J9vlv19VT1TV\n",
       "RuBW4ONVtamqvgp8nCapkjSefhDYq6p+u6qeqqp7gQ8Ap1bVGuCvgE8DxwNvnrDt31TV31fV/wP+\n",
       "O/BDSQ6gWyy6tKruqar/AK4EDmufOx34WFV9AqCqPgXc1O5z2LZDT7W1dXq0rdd7gefw7PiqRcrE\n",
       "SdN5eELHxq8Du3XYbj++1Yq0zb+0y7e5b5Lttg7M//skj7uULWlpejGwX3ta7NEkjwLn0bTmQNMC\n",
       "/XKaH12PDmxXwOZnHlT9G/AITbzZl+GxaMvA/GCceTHw0xPqcxRNn85h2w6V5NeTbGxPIT4K7E7T\n",
       "YqZFzsRp+/Z1YNeBx/vy7FN1s3U/TdAZ9OJ2+TZezilp0JeBe6vqhQPTC6rqxCQ7An8K/DlwdpKX\n",
       "DGwXYMUzD5LdaE7L3Q98heGxaLr6XDahPs+vqt/vsO208S3J0cBvAD9dVXu0pwIfx07hS4KJ0/Zt\n",
       "A/CGJDsmOR744RlsO90X/GPAIUlOS7IsySnAd9P0F+iy/WTrGFCk8bYO+FqS30zy3DYufU+SHwTe\n",
       "DnwTeBPwv4A/b/tEbfPaJEe1fYTeDfxTVd1Pc4p/trHog8BPJDmurcsu7UUz+3fY9iHgaeAlUzz/\n",
       "fOAp4F+T7JzkHTT9SbUEmDht384BfgJ4lKZz5dUTnp/uV9PEjuTPzFfVI8CJwK8B/0rTYfLEdvlU\n",
       "+56srIn7t5VKGlNtt4ATafoJfYkm+fhT4BiaTuI/V83Ag79HEwvetm1Tms7V5wMP0/SFPL3d58PM\n",
       "LBY9E2eqajNwMk3S9iBNC9Sv8exkaaptvw78T+AfkjyS5JU8O4Z9op3uAjbRnOb78mT70uLTaQDM\n",
       "JHvQdNJ7Oc2b+SbgbuAvaZo9NwGvr6rHequpJE0iyS7A39J0rt0Z+OuqOq+9omrSGJXkPJortb4J\n",
       "vLWqrluIumvuklwKbK6q31roumj70LXF6Y9ori54GfC9NFclnAusqapDgOvbx5I0Uu0VTcdU1WE0\n",
       "8emYdkycSWNUkkOBU2iG0TgeeN+E0z5aWjyNr5EaGizacXuOrqpLANrLRB+nuZxzdbvaauB1vdVS\n",
       "kqbRnhqBpsVpR5rTz1PFqJOBK6rqyaraBNwDHDG62mqeeVpLI7WswzoHAQ+1zaHfB3ye5nzz8qra\n",
       "drn4VprBEiVp5NoWoy/QdMa9qKpuTzJVjNoPGByQcDMw2OFXS0hVvWmh66DtS5fEaRnw/cAvVdXn\n",
       "klzAhNNyVVVJvi3jn2yZpPFWVSM/ddJ2LD6sbSH/ZJJjJjw/aYwaXGXiAuOXtH0aFsO6JE6baTre\n",
       "fa59/GGaQcm2JNmnqrYk2ZfmqoMZV2ApSLKqqlYtdD3mg8eyOI3LsSx0slFVjyf5KPADwNYpYtT9\n",
       "DIz7AxzAFOP6jDJ+jfozsBCfuXE/Rstb+mV2iWFD+zhV1RbgviSHtIuOBW6nuafQGe2yM4BrZllP\n",
       "SZq1JHu1V/6S5LnAj9Hc7+xaJo9R1wKntuPnHAQcTDOGkCQN1aXFCeAtwOXt4GJfpBmOYEfgyiRn\n",
       "0l7q20sNF4lR/ZIehxY6acT2BVa3/Zx2oBnt+fok65kkRlXVxiRXAhtpBiE8q7qMyyJJdEycqupm\n",
       "mhswTnTs/FZn0VoLnN//hRsjyZnWjqKQEVm70BWYR2sXugJLVVXdStMPc+LyR5giRlXVe4D39Fy1\n",
       "mVo75uUtRJmWt7TLW6gyp9VpAMxZ7zypcWlBaVqc+k+cxuX10vZp3L7z43Iskrrp8r130DdJkqSO\n",
       "TJwkSZI6MnGSJEnqqOtVddJ2yysqJUnbmDhJnYzFFZWSpDnyVJ0kSVJHtjhJ0hSS9DVW3VerytHK\n",
       "pSXIcZw6chyn7ZfvfXfj951/5ePzv+ev7Qibv1z1+Mvnf9+S5qJLDLPFSdpOLPQNeJemz+4+//tc\n",
       "B5xg7JWWKL+80nbFTu6SNBe9J05JTu+7DOCmqrpzBOVIkqTt2AhanH70T2D3p/vb/+d3gn/5TcDE\n",
       "SZIk9WoEidMf7wqH9rj/s74BF/W4f0mSpIbjOEmSJHVk4iRJktSRiZMkSVJHJk6SJEkdOY6TtEg4\n",
       "QKUkLX6dWpySbEpyS5L1Sda1y/ZMsibJXUmuS7JHv1WVxl31PI2vJCuS3JDk9iS3JXlru3xVks1t\n",
       "7Fqf5ISBbc5LcneSO5Mct3C1l7SUdG1xKmBlVT0ysOxcYE1V/X6St7WPz53vCkpSB08Cv1JVG5Ls\n",
       "Bnw+yRqa2PXeqnrv4MpJDgVOoRkrZX/gU0kOqaoex5yTNA5m0sdp4r0UTgJWt/OrgdfNS41m54+S\n",
       "VJ/TAh6bpCGqaktVbWjnnwDuoEmIYPL7wJwMXFFVT1bVJuAe4IhR1FXS0tY1cSqaX2Q3JfmFdtny\n",
       "qtrazm8Fls977WbE0xySIMmBwOHAZ9tFb0lyc5KLB7oU7AdsHthsM99KtCRpSl1P1R1VVQ8k2RtY\n",
       "k+RZtzepqmlaZS4E9m7nV7aTpPGwtp0Wh/Y03YeBc6rqiSQXAe9qn3438AfAmVNsPkkMWzUwvxLj\n",
       "lzRekqxkhl/sTolTVT3Q/n0oydU0Tdpbk+xTVVuS7As8OPnWZ9PvLVckLZyVPDvmvHNhqgEk2Qm4\n",
       "CvhgVV0DUFUPDjz/AeAj7cP7gRUDmx/QLptgVT+VlbQoVNVaBn79JTl/2DZDT9Ul2TXJ89v55wHH\n",
       "AbcC1wJntKudAVwz4xpL0jxIEuBiYGNVXTCwfN+B1X6SJnZBE79OTbJzkoOAg4F1o6qvpKWrS4vT\n",
       "cuDqJi6xDLi8qq5LchNwZZIzgU3A63urpSRN7yjgdOCWJOvbZW8HTktyGM1puHuBNwNU1cYkVwIb\n",
       "gaeAs6rKzoyShhqaOFXVvcBhkyx/BDi2j0pJ0kxU1d8zeQv6x6fZ5j3Ae3qrlKSx5C1XJEmSOjJx\n",
       "kiRJ6sjESZIkqSMTJ0mSpI66DoCpERnV7V2qarLbUEiSpGmYOC06o8ibzJkkSZoNEyf1wpYzSdI4\n",
       "MnFSj/rOncyZJEmjZedwSZKkjkycJEmSOjJxkiRJ6sjESZIkqSMTJ0mSpI5MnCRJkjoycZIkSerI\n",
       "xEmSJKkjEydJkqSOTJwkSZI6MnGSJEnqqFPilGTHJOuTfKR9vGeSNUnuSnJdkj36raYkTS3JiiQ3\n",
       "JLk9yW1J3tounzJWJTkvyd1J7kxy3MLVXtJS0rXF6RxgI9+6a+u5wJqqOgS4vn0sjVyS6nta6GNU\n",
       "J08Cv1JVLweOBM5O8jKmiFVJDgVOAQ4Fjgfel8QWeElDDQ0USQ4AXgt8gG/djv4kYHU7vxp4XS+1\n",
       "k4aqEUxa7KpqS1VtaOefAO4A9mfqWHUycEVVPVlVm4B7gCNGWmlJS1KXX1h/CPwG8PTAsuVVtbWd\n",
       "3wosn++KSdJsJDkQOBy4kalj1X7A5oHNNtMkWpI0rWXTPZnkRODBqlqfZOVk61TVkNMZFwJ7t/Mr\n",
       "20nSeFjbTotDkt2Aq4BzquprSZ55bnismqx5cdXA/EqMX9J4aXOblTPZZtrECXgVcFKS1wK7AC9I\n",
       "chmwNck+VbUlyb7Ag1Pv4myabgSSxs9Knh1z3rkw1QCS7ESTNF1WVde0i6eKVfcDKwY2P6BdNsGq\n",
       "3uoraeFV1VoGfv0lOX/YNtOeqquqt1fViqo6CDgV+HRV/SxwLXBGu9oZwDVT7UOS+pamaeliYGNV\n",
       "XTDw1FSx6lrg1CQ7JzkIOBhYN6r6Slq6hrU4TbStKft3gSuTnAlsAl4/n5WSpBk6CjgduCXJ+nbZ\n",
       "eUwRq6pqY5Iraa4Wfgo4q6q8EkDSUOkzVjT9CW6n31N1Z30DLnpO/1c/hfEooymnqjJ8vTmUkNQ4\n",
       "vV7jcSyjKaPvz9ao9PcZXgeccFfVwy/tYeeS5iBJDYthjlsiSZLU0UxP1WlMOLCjJEkzZ+K03RrF\n",
       "aSFJksaLp+okSZI6MnGSJEnqyMRJkiSpIxMnSZKkjkycJEmSOjJxkiRJ6sjESZIkqSMTJ0mSpI5M\n",
       "nCRJkjoycZIkSerIxEmSJKkjEydJkqSOTJwkSZI6MnGSJEnqyMRJkiSpIxMnSZKkjqZNnJLskuTG\n",
       "JBuSbEzyO+3yPZOsSXJXkuuS7DGa6krSt0tySZKtSW4dWLYqyeYk69vphIHnzktyd5I7kxy3MLWW\n",
       "tBRNmzhV1X8Ax1TVYcD3AsckeTVwLrCmqg4Brm8fS9JCuRQ4fsKyAt5bVYe308cBkhwKnAIc2m7z\n",
       "viS2vkvqZGiwqKqvt7M7AzsCjwInAavb5auB1/VSO0nqoKo+QxObJsoky04GrqiqJ6tqE3APcESP\n",
       "1ZM0RoYmTkl2SLIB2ArcUFW3A8uramu7ylZgeY91lKTZekuSm5NcPNClYD9g88A6m4H9R181SUvR\n",
       "smErVNXTwGFJdgc+meSYCc9Xkpp6DxcCe7fzK9tJ0nhY206L0kXAu9r5dwN/AJw5xbpTxLBVA/Mr\n",
       "MX5J4yXJSmb4xR6aOG1TVY8n+SjwA8DWJPtU1ZYk+wIPTr3l2TRdCSSNn5U8O+a8c2GqMYmqeiYu\n",
       "JfkA8JH24f3AioFVD2iXTWJVP5WTtChU1VoGfv0lOX/YNsOuqttrW/N2kucCPwasB64FzmhXOwO4\n",
       "ZlY1lqSetD/qtvlJYNsVd9cCpybZOclBwMHAulHXT9LSNKzFaV9gdXvFyQ7AZVV1fZL1wJVJzgQ2\n",
       "Aa/vt5qSNLUkVwCvAfZKch9wPrAyyWE0p+HuBd4MUFUbk1wJbASeAs6qqmm6G0jSt6TPeNH0fbqd\n",
       "fk/VnfUNuOg5U3ZRmDdhPMoYVTnjUsaoyhmfMqpqsivZlpwmfvXxeq0DTrir6uGX9rBzSXOQpIbF\n",
       "MMcukSRJ6sjESZIkqSMTJ0mSpI5MnCRJkjoycZIkSerIxEmSJKkjEydJkqSOTJwkSZI6MnGSJEnq\n",
       "yMRJkiSpIxMnSZKkjkycJEmSOlq20BWQpO3PI4c0NxHux7jcaFlajEycJGlB9JU3mTNJffJUnSRJ\n",
       "UkcmTpIkSR2ZOEmSJHVk4iRJktSRiZMkSVJHQxOnJCuS3JDk9iS3JXlru3zPJGuS3JXkuiR79F9d\n",
       "Sfp2SS5JsjXJrQPLpoxRSc5LcneSO5MctzC1lrQUdWlxehL4lap6OXAkcHaSlwHnAmuq6hDg+vax\n",
       "JC2ES4HjJyybNEYlORQ4BTi03eZ9SWx9l9TJ0GBRVVuqakM7/wRwB7A/cBKwul1tNfC6viopSdOp\n",
       "qs8Aj05YPFWMOhm4oqqerKpNwD3AEaOop6Slb0a/spIcCBwO3Agsr6qt7VNbgeXzWjNJmpupYtR+\n",
       "wOaB9TbT/BiUpKE6jxyeZDfgKuCcqvpa8q3Raauqpr59wIXA3u38ynaSNB7WttPiNn2MalaZfPGq\n",
       "gfmVGL+k8ZJkJTP8YndKnJLsRJM0XVZV17SLtybZp6q2JNkXeHDyrc+m6Uogafys5Nkx550LU43J\n",
       "TRWj7gdWDKx3QLtsEqv6rJ+kBVZVaxn49Zfk/GHbdLmqLsDFwMaqumDgqWuBM9r5M4BrJm4rSQto\n",
       "qhh1LXBqkp2THAQcDKxbgPpJWoK6tDgdBZwO3JJkfbvsPOB3gSuTnAlsAl7fSw0laYgkVwCvAfZK\n",
       "ch/wDqaIUVW1McmVwEbgKeCsqurrjruSxkz6jBdNn4Lb6fdU3VnfgIue09+dxrcJ41HGqMoZlzJG\n",
       "Vc74lFFVGb7e4tfErz5er3XAK+nvvRif90AatSQ17Pvj2CWSJEkdmThJkiR1ZOIkSZLUkYmTJElS\n",
       "RyZOkiRJHZk4SZIkdWTiJEmS1JGJkyRJUkcmTpIkSR2ZOEmSJHVk4iRJktSRiZMkSVJHJk6SJEkd\n",
       "mThJkiR1ZOIkSZLUkYmTJElSRyZOkiRJHZk4SZIkdWTiJEmS1NHQxCnJJUm2Jrl1YNmeSdYkuSvJ\n",
       "dUn26LeakjQ7STYluSXJ+iTr2mXGMEmz0qXF6VLg+AnLzgXWVNUhwPXtY0lajApYWVWHV9UR7bKx\n",
       "jmFJqq9poY9NWmhDE6eq+gzw6ITFJwGr2/nVwOvmuV6SNJ8y4fGYx7DqaZI02z5Oy6tqazu/FVg+\n",
       "T/WRpPlWwKeS3JTkF9plxjBJs7JsrjuoqiHNtxcCe7fzK9tJ0nhY206L2lFV9UCSvYE1Se4cfHL6\n",
       "GLZqYH4lxi9pvCRZyQy/2LNNnLYm2aeqtiTZF3hw6lXPBg6dZTGSFreVPDvmvHNhqjGNqnqg/ftQ\n",
       "kquBI+gcw1aNqpqSFkBVrWXg11+S84dtM9tTddcCZ7TzZwDXzHI/ktSbJLsmeX47/zzgOOBWjGGS\n",
       "Zmloi1OSK4DXAHsluQ94B/C7wJVJzgQ2Aa/vs5KSNEvLgauTQBPvLq+q65LchDFM0iwMTZyq6rQp\n",
       "njp2nusiSfOqqu4FDptk+SMYwyTNgiOHS5IkdWTiJEmS1JGJkyRJUkcmTpIkSR2ZOEmSJHVk4iRJ\n",
       "ktSRiZMkSVJHJk6SJEkdzfkmv5Kk7cf0N3Wfm6pKX/uW5ouJkyRpBvrKm8yZtDR4qk6SJKkjEydJ\n",
       "kqSOTJwkSZI6MnGSJEnqyMRJkiSpIxMnSZKkjkycJEmSOnIcJ0nSotDn4JrgAJuaHyZOkqRFos+8\n",
       "yZxJ82NOp+qSHJ/kziR3J3nbfFVq8Vm70BWYR2sXugLSorD44tfaMS9vIcocbXlJVlre0i9zmFkn\n",
       "Tkl2BP43cDxwKHBakpfNV8UWl7ULXYF5tHahKyAtuMUZv9aOeXkLUeaoy2Ol5Y1FmdOay6m6I4B7\n",
       "qmoTQJIPAScDd8xDvSSpT8av7VDffajaMs6f733aN2txmUvitD9w38DjzcArv321Nz4Bu31zDuUM\n",
       "cedz+tu3pDHVMX79yOPzX/Tjy4Dnzf9+NVyfNyguYFU7ze++p0v45pqo9ZWUzWeSOtkxLmQyOZfE\n",
       "qeOL8rnd5lDGDIziNRyXMkZVzriUMapyxqWMJaFj/Lph9/6qMNl78c4e9z2Z2ZQ318/QdGX28fkc\n",
       "LK/Pz/+2fc/XezgaM01w+mhRm41RtB5OZS6J0/3AioHHK2h+tT3D5kVJi5TxS9KszOWqupuAg5Mc\n",
       "mGRn4BTg2vmpliT1yvglaVZm3eJUVU8l+SXgk8COwMVVZcdKSYue8UvSbKVqwU4TSpIkLSlzvldd\n",
       "l0Hkkvxx+/zNSQ6fa5l9GXYsSd7QHsMtSf4hyfcuRD276Dq4X5IfTPJUkv86yvrNRMfP2Mok65Pc\n",
       "lmTtiKvYSYfP115JPpFkQ3scb1yAag6V5JIkW5PcOs06S+I7P5VRDo7Z5fWc5/JWJLkhye3t5+yt\n",
       "PZe3S5Ib28/1xiS/02d5A+Xu2MaEj4yovE3t/4b1SdaNoLw9knw4yR3t63pkj2W9tD2ubdPjI/jc\n",
       "nNd+Rm9N8hdJer2CPsk5bVm3JTln2pWratYTTRP3PcCBwE7ABuBlE9Z5LfCxdv6VwGfnUmZfU8dj\n",
       "+SFg93b++KV8LAPrfRr4G+CnFrrec3hf9gBuBw5oH++10PWe5XGsAn5n2zEADwPLFrrukxzL0cDh\n",
       "wK1TPL8kvvNzea9G+Xr2UN4+wGHt/G7AP/d5fG05u7Z/lwGfBV49guP8VeBy4NoRva73AnuOoqy2\n",
       "vNXAzw+8rruPqNwdgAeAFT2WcSDwJeA57eO/BM7osbzvAW4Fdmm//2uAl0y1/lxbnJ4ZRK6qngS2\n",
       "DSI36KT2DaaqbgT2SLJ8juX2YeixVNU/VdW2cV1uBA4YcR276vK+ALwF+DDw0CgrN0NdjuVngKuq\n",
       "ajNAVf3riOvYRZfjeAB4QTv/AuDhqnpqhHXspKo+Azw6zSpL5Ts/la7fn3nR4fWc7/K2VNWGdv4J\n",
       "mkE/9+u5zK+3szvT/GN6pM/ykhxAk8B/gNGOwTGSspLsDhxdVZdA02dv4H9T344FvlhV9w1dc/a+\n",
       "CjwJ7JpkGbArzZWwfflu4Maq+o+q+ibwt8CUZ2HmmjhNNojc/h3WWYwJR5djGXQm8LFeazR7Q48l\n",
       "yf40/wwuahct1s5uXd6Xg4E929MPNyX52ZHVrrsux/F+4OVJvgLcDEzfXLx4LZXv/FRmGguWrCQH\n",
       "0rR23dhzOTsk2QBsBW6oqo19lgf8IfAbwNM9lzOogE+1MegXei7rIOChJJcm+UKS9yfZtecytzkV\n",
       "+Is+C6iqR4A/AL4MfAV4rKo+1WORtwFHJ9mzfR1/nGli1lwTp67/bCdm4Yvxn3TnOiU5Bvh5YBHc\n",
       "GHRSXY7lAuDcatopw+IdGbHLsewEfD/NL8z/AvxWkoN7rdXMdTmOtwMbqmo/4DDgwiTP77davVkK\n",
       "3/mpLKW6zlqS3WhanM9pW556U1VPV9VhNP+Mfjg93rg1yYnAg1W1ntHGtaOq6nDgBODsJEf3WNYy\n",
       "mpj3vqr6fuDfgHN7LA+AduiOnwD+qudyXgL8Ms0pu/2A3ZK8oa/yqupO4PeA64CPA+uZJumea+I0\n",
       "dBC5SdY5gH6b3Gary7HQdgh/P3BSVY2seX2GuhzLDwAfSnIv8FPA+5KcNKL6zUSXY7kPuK6q/r2q\n",
       "Hgb+Dvi+EdWvqy7H8SragFRVX6TpM/HSkdRufi2V7/xUOsWCpSzJTsBVwAer6ppRldueTvoo8Ioe\n",
       "i3kVcFIb264AfiTJn/dYHgBV9UD79yHgappTvn3ZDGyuqs+1jz9Mk0j17QTg8+0x9ukVwD9W1bbu\n",
       "Cv+X5n3tTVVdUlWvqKrXAI/R9P2b1FwTpy6DyF0L/BxA2+v/saraOsdy+zD0WJK8iOYNPL2q7lmA\n",
       "OnY19Fiq6ruq6qCqOojmS/eLVbUYBwDs8hn7a+DV7VU0u9J0SO77VMBMdTmOO2n6D9D2CXopTQfJ\n",
       "pWapfOenMtaDYyYJcDGwsaouGEF5eyXZo51/LvBjNL/oe1FVb6+qFW1sOxX4dFX9XF/lASTZdVvr\n",
       "cJLnAcfRdDbuRVVtAe5Lcki76FiaC2T6dhpNMtq3O4Ejkzy3/bweS88xPcl3tn9fBPwk05yOnMst\n",
       "V6gpBpFL8ub2+T+pqo8leW2Se2iaE980lzL70uVYgHcALwQuat5LnqyqPn9VzErHY1kSOn7G7kzy\n",
       "CeAWmubV94+gD8WMdHxP3gNcmuRmmh81v9me619UklwBvAbYK8l9wPk0p0uX1Hd+KlO9V32VN/B6\n",
       "fkf7er6jqi7tqzzgKOB04JYk2xKY86rqEz2Vty+wOskONJ/ry6rq+p7KmswoTr0uB65u/y8sAy6v\n",
       "qut6LvMtwOVtcv9Fev6etQnhsUDf/beoqpvbVsKbaGL6F4A/7bnYDyf5DppO6WdV1VenWtEBMCVJ\n",
       "kjqa8wCYkiRJ2wsTJ0mSpI5MnCRJkjoycZIkSerIxEmSJKkjEydJkqSOTJwkSZI6+v8+SnISQC+O\n",
       "ngAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f738c0220d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "l = 1 # lambda\n",
    "u = np.random.uniform(0, 1, (500,))\n",
    "e = -np.log(1-u) / l\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(u)\n",
    "plt.title('uniform')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(e)\n",
    "_ = plt.title('exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! But what if we want to generate pictures like those in the MNIST dataset? Can we estimate something like and inverse CDF in that case? Well, it happens that this is a hard problem and it has an entire field dedicated to it. That field is called Generative Modelling by the machine learning community. If you remember Restricted Boltzman Machines (RBM), Autoencoders (AE), Sparse Coding, etc, that's what I'm talking about. Here we are interested in a generative model proposed by Goodfellow et. al [1] called Generative Adversarial Networks (GAN).\n",
    "\n",
    "GANs propose to generate samples from a dataset using (guess what?) adaptive neural networks. Thus, given ac uniformely distributed vector $\\mathbf{U}$, the generated samples are given by $\\mathbf{X} = G(\\mathbf{U})$. Which doesn't quite solve the problem before we define how to train the DNN $G(\\cdot)$. Let us think about that next, following Goodfellow et. al [1].\n",
    "\n",
    "Given a dataset $S$, and a test statistic $D(\\cdot)$, we say that a sample $\\mathbf{X}$ comes from the underlying PDF that generated the samples in $S$ if $D(\\mathbf{X}) = 1$. We reject that hypothesis if $D(\\mathbf{X}) = 0$. I would give you a dollar if you can guess how the guys from Bengio's lab proposed to parameterize $D(\\cdot)$... \n",
    "\n",
    "WRONG! They used DNNs. \n",
    "\n",
    "As a note, their derivations are general enough, and we can use other adaptive models instead. That being said, if $G$ wants to be good enough it must pass the test defined by $D$. On the other hand, $D$ must play it hard and only pass the real samples coming from $S$ itself. This way one can define a min-max game where the cost function for $G$ is the binary crossentropy between $D(G(\\mathbf{U}))$ and 1, i.e\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_G = log(D(G(\\mathbf{U}))),\n",
    "\\end{equation}\n",
    "\n",
    "the cost function for $D$, on the other hand, is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_D = \\begin{cases}\n",
    "        \\begin{array}{lcl}\n",
    "        log(D(\\mathbf{X})), \\quad\\quad if: \\mathbf{X}\\in S \\\\\n",
    "        log(1 - D(\\mathbf{X})), \\quad if: \\mathbf{X}=G(\\mathbf{U}).\n",
    "        \\end{array}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the final solution (minimum for the combined cost functions above) should give us realistic looking samples $G(\\mathbf{U})$ and a totally clueless $D$ returning $D(X) = \\frac{1}{2}$. Hopefully we understood that well enough to hack something up with Keras. Let us do that for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These first lines are a copy paste from keras/examples/mnist_mlp.py\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation, Reshape, Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.advanced_activations import LeakyReLU as lrelu\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our model for $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = 3000  # dim of the random number vector\n",
    "mnist_dim = 784\n",
    "\n",
    "detector = Sequential()\n",
    "detector.add(Dense(dim, input_dim=mnist_dim))\n",
    "detector.add(lrelu())\n",
    "detector.add(Dropout(.3))\n",
    "detector.add(Dense(dim))\n",
    "detector.add(Activation('tanh'))\n",
    "detector.add(Dropout(.3))\n",
    "detector.add(Dense(1)) # 1: Yes, it belongs to S, 0: fake!\n",
    "detector.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a copy of $D$ that we should use to adapt the cost function for $G$. Note that this copy should not be adapted with gradients coming from $\\mathcal{L}_G$, otherwise $D$ will be cheating and we won't learn anything useful. To do that with Keras, we simply set `trainable=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can define $G$ using our hacked detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fully Connected model\n",
    "\n",
    "sampler = Sequential()\n",
    "sampler.add(Dense(dim, input_dim=dim))\n",
    "sampler.add(lrelu())\n",
    "sampler.add(Dense(dim))\n",
    "sampler.add(lrelu())\n",
    "sampler.add(Dense(mnist_dim))\n",
    "sampler.add(Activation('sigmoid'))\n",
    "\n",
    "# This is G itself!!!\n",
    "sample_fake = theano.function([sampler.get_input()], sampler.get_output())\n",
    "\n",
    "# We add the detector G on top, but it won't be adapted with this cost function.\n",
    "# But here is a dirty hack: Theano shared variables on the GPU are the same for \n",
    "# `detector` and `detector_no_grad`, so, when we adapt `detector` the values of \n",
    "# `detector_no_grad` will be updated as well. But this only happens following the \n",
    "# correct gradients. \n",
    "# Don't you love pointers? Aliasing can be our friend sometimes.\n",
    "detector.trainable = False\n",
    "sampler.add(detector)\n",
    "\n",
    "opt_g = Adam(lr=.001) # I got better results when \n",
    "                      # detector's learning rate is faster\n",
    "sampler.compile(loss='binary_crossentropy', optimizer=opt_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "opt_d = Adam(lr=.002)\n",
    "detector.trainable = True\n",
    "detector.compile(loss='binary_crossentropy', optimizer=opt_d)\n",
    "detector.predict(np.ones((3, mnist_dim))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can train our networks. First we sample a uniform random vector $\\mathbf{U}$, using that we sample fake images with $G(\\mathbf{U})$. We train $G$ to get passing values in the $D$ test, I mean 1. Later, we concatenate the fake images with some real MNIST images and train $D$ to return zeros for the formers and ones for the later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 1000 # it takes some time to get something recognizable.\n",
    "batch_size = 128\n",
    "num_batches = X_train.shape[0] / 128\n",
    "fig = plt.figure()\n",
    "fixed_noise = np.random.uniform(-1, 1, (9, dim)).astype('float32') # Let us visualize how these sampes evolve with training\n",
    "\n",
    "progbar = generic_utils.Progbar(1000)\n",
    "try:\n",
    "    for e in range(nb_epoch):\n",
    "        loss0 = 0\n",
    "        loss1 = 0\n",
    "\n",
    "        for (first, last) in zip(range(0, X_train.shape[0]-batch_size, batch_size),\n",
    "                                 range(batch_size, X_train.shape[0], batch_size)):\n",
    "            noise_batch = np.random.uniform(-1, 1, (batch_size, dim)).astype('float32')\n",
    "            fake_samples = sample_fake(noise_batch)\n",
    "            true_n_fake = np.concatenate([X_train[first: last],\n",
    "                                          fake_samples], axis=0)\n",
    "            y_batch = np.concatenate([np.ones((batch_size, 1)),\n",
    "                                      np.zeros((batch_size, 1))], axis=0).astype('float32')\n",
    "            all_fake = np.ones((batch_size, 1)).astype('float32')\n",
    "            # We take turns adapting G and D. We may give D an upper hand,\n",
    "            #  letting it train for more turns, keeping G fixed.\n",
    "            #  Do that increasing the upper hand `uh` variable.\n",
    "            uh = 2\n",
    "            if e % uh == 0:\n",
    "                loss0 += sampler.train_on_batch(noise_batch, all_fake)[0]\n",
    "            else:\n",
    "                loss1 += detector.train_on_batch(true_n_fake, y_batch)[0]\n",
    "            loss = loss0 + loss1\n",
    "\n",
    "        progbar.add(1, values=[(\"train loss\", loss),\n",
    "                               (\"G loss\", loss0),\n",
    "                               (\"D loss\", loss1)])\n",
    "\n",
    "        if e % 10 == 0: # visualize results once in a while\n",
    "            fixed_fake = sample_fake(fixed_noise)\n",
    "            plt.clf()\n",
    "            for i in range(9):\n",
    "                plt.subplot(3, 3, i+1)\n",
    "                plt.imshow(fixed_fake[i].reshape((28,28)), cmap='gray')\n",
    "                plt.axis('off')\n",
    "            fig.canvas.draw()\n",
    "            plt.show()\n",
    "            plt.savefig(str(e)+'.png')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Convolutional GAN\n",
    "\n",
    "As an easy exercise for the reader (best way to look knowledgeable when your lecture notes aren't finished yet), I recommend you to modify this code to parameterize $G$ and $D$ as convolutional networks. Check [DCGAN](https://github.com/Newmu/dcgan_code) out for inspiration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
